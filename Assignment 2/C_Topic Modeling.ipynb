{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import re,string\n",
    "import nltk\n",
    "from patsy import dmatrices\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('insta_caption.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Engagement_score</th>\n",
       "      <th>photo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>happypills</th>\n",
       "      <th>pills</th>\n",
       "      <th>pharma</th>\n",
       "      <th>bigpharma</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>tree</th>\n",
       "      <th>turnthetide</th>\n",
       "      <th>amish</th>\n",
       "      <th>farm</th>\n",
       "      <th>ngari</th>\n",
       "      <th>zanda</th>\n",
       "      <th>idg2018</th>\n",
       "      <th>dayofthegirl</th>\n",
       "      <th>unicef</th>\n",
       "      <th>educategirls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.635760</td>\n",
       "      <td>1</td>\n",
       "      <td>['happypills', 'pills', 'pharma', 'bigpharma']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.075393</td>\n",
       "      <td>1</td>\n",
       "      <td>['1', '2']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.381748</td>\n",
       "      <td>1</td>\n",
       "      <td>['FollowMe', 'Madagascar', 'Enoughness', 'natu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.064558</td>\n",
       "      <td>1</td>\n",
       "      <td>['snowstorm', 'penguin', 'antarctica']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.036183</td>\n",
       "      <td>1</td>\n",
       "      <td>['whales', 'humpbackwhales', 'parenting', 'pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Engagement_score  photo  \\\n",
       "0           0         -0.635760      1   \n",
       "1           1         -0.075393      1   \n",
       "2           2         -0.381748      1   \n",
       "3           3          1.064558      1   \n",
       "4           4          0.036183      1   \n",
       "\n",
       "                                            hashtags  happypills  pills  \\\n",
       "0     ['happypills', 'pills', 'pharma', 'bigpharma']           1      1   \n",
       "1                                         ['1', '2']           0      0   \n",
       "2  ['FollowMe', 'Madagascar', 'Enoughness', 'natu...           0      0   \n",
       "3             ['snowstorm', 'penguin', 'antarctica']           0      0   \n",
       "4  ['whales', 'humpbackwhales', 'parenting', 'pla...           0      0   \n",
       "\n",
       "   pharma  bigpharma  1  2      ...       tree  turnthetide  amish  farm  \\\n",
       "0       1          1  0  0      ...          0            0      0     0   \n",
       "1       0          0  1  1      ...          0            0      0     0   \n",
       "2       0          0  0  0      ...          0            0      0     0   \n",
       "3       0          0  0  0      ...          0            0      0     0   \n",
       "4       0          0  0  0      ...          0            0      0     0   \n",
       "\n",
       "   ngari  zanda  idg2018  dayofthegirl  unicef  educategirls  \n",
       "0      0      0        0             0       0             0  \n",
       "1      0      0        0             0       0             0  \n",
       "2      0      0        0             0       0             0  \n",
       "3      0      0        0             0       0             0  \n",
       "4      0      0        0             0       0             0  \n",
       "\n",
       "[5 rows x 1239 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(997, 1239)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_captions = pd.DataFrame(df[['hashtags','Engagement_score']], index= df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_captions.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pros.dropna(axis=0, inplace=True)\n",
    "#cons.dropna(axis=0, inplace=True)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "punc = string.punctuation\n",
    "\n",
    "def clean_tokenize(s):\n",
    "        s = re.sub(r'[^\\w\\s]', '',s.lower())\n",
    "        return([word for word in word_tokenize(s) if word not in stop if word not in punc])\n",
    "def get_lemma(word):\n",
    "        lemma = wordnet.morphy(word)\n",
    "        if lemma is None:\n",
    "            return word\n",
    "        else:\n",
    "            return lemma\n",
    "        def get_lemma2(word):\n",
    "            return WordNetLemmatizer().lemmatize(word)\n",
    "def lemmatize(my_tokens):\n",
    "    \"\"\"Function to enable data preparation for topic modeling using LDA, \n",
    "    includes removing stop words, lemmatization, tokenization\"\"\"\n",
    "    my_tokens = [get_lemma(token) for token in my_tokens]\n",
    "    return my_tokens\n",
    "\n",
    "# def dataprep(text_data_df):\n",
    "#     pros = pd.DataFrame(text_data_df['Pros'], index= text_data_df.index)\n",
    "#     cons = pd.DataFrame(text_data_df['Cons'],  index= text_data_df.index)\n",
    "#     pros.dropna(axis=0, inplace=True)\n",
    "#     cons.dropna(axis=0, inplace=True)\n",
    "#     stop = set(stopwords.words('english'))\n",
    "#     punc = string.punctuation\n",
    "    \n",
    "#     pros['pros_tokens'] =pros.Pros.map(clean_tokenize)\n",
    "#     cons['cons_tokens'] =cons.Cons.map(clean_tokenize)\n",
    "#     #my_tokens = [get_lemma(token) for token in my_tokens]\n",
    "#     pros['pros_lemma_tokens'] = pros['pros_tokens'].apply(lemmatize)\n",
    "#     #pros['pros_lemma_tokens'] = [get_lemma(token) for token in pros['pros_tokens']]\n",
    "#     cons['cons_lemma_tokens'] = cons['cons_tokens'].apply(lemmatize)\n",
    "#     return pros, cons\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_captions['clean_tokens'] =insta_captions.hashtags.map(clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Engagement_score</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['happypills', 'pills', 'pharma', 'bigpharma']</td>\n",
       "      <td>-0.635760</td>\n",
       "      <td>[happypills, pills, pharma, bigpharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['1', '2']</td>\n",
       "      <td>-0.075393</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['FollowMe', 'Madagascar', 'Enoughness', 'natu...</td>\n",
       "      <td>-0.381748</td>\n",
       "      <td>[followme, madagascar, enoughness, nature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['snowstorm', 'penguin', 'antarctica']</td>\n",
       "      <td>1.064558</td>\n",
       "      <td>[snowstorm, penguin, antarctica]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['whales', 'humpbackwhales', 'parenting', 'pla...</td>\n",
       "      <td>0.036183</td>\n",
       "      <td>[whales, humpbackwhales, parenting, planetofth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hashtags  Engagement_score  \\\n",
       "0     ['happypills', 'pills', 'pharma', 'bigpharma']         -0.635760   \n",
       "1                                         ['1', '2']         -0.075393   \n",
       "2  ['FollowMe', 'Madagascar', 'Enoughness', 'natu...         -0.381748   \n",
       "3             ['snowstorm', 'penguin', 'antarctica']          1.064558   \n",
       "4  ['whales', 'humpbackwhales', 'parenting', 'pla...          0.036183   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0             [happypills, pills, pharma, bigpharma]  \n",
       "1                                             [1, 2]  \n",
       "2         [followme, madagascar, enoughness, nature]  \n",
       "3                   [snowstorm, penguin, antarctica]  \n",
       "4  [whales, humpbackwhales, parenting, planetofth...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insta_captions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(insta_captions['clean_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in insta_captions['clean_tokens']]\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.064*\"followme\" + 0.028*\"nature\" + 0.019*\"antarctica\" + 0.017*\"ocean\" + 0.016*\"wildlife\"')\n",
      "(1, '0.025*\"simonnorfolk\" + 0.023*\"documentaryphotography\" + 0.014*\"photojournalism\" + 0.014*\"everydayrefugees\" + 0.013*\"archaeology\"')\n",
      "(2, '0.031*\"okavangolions\" + 0.020*\"thisismytrophy\" + 0.019*\"nature\" + 0.019*\"wildlife\" + 0.017*\"tsaropride\"')\n",
      "(3, '0.028*\"onassignment\" + 0.021*\"muhammedmuheisen\" + 0.020*\"stephenwilkes\" + 0.018*\"daytonight\" + 0.015*\"photojournalism\"')\n",
      "(4, '0.033*\"kenya\" + 0.026*\"africa\" + 0.018*\"photojournalism\" + 0.018*\"nature\" + 0.015*\"northernkenya\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Topics\n",
    "\n",
    "* Landscapes\n",
    "* Refugees\n",
    "* Large Wildcats\n",
    "* Top NatGeo Photographers\n",
    "* Africa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering by Engagement Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Engagement_score</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>['blackrhino', 'poachers', 'Ivory']</td>\n",
       "      <td>14.102355</td>\n",
       "      <td>[blackrhino, poachers, ivory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>[]</td>\n",
       "      <td>12.622301</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>[]</td>\n",
       "      <td>4.637615</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>[]</td>\n",
       "      <td>4.221715</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>[]</td>\n",
       "      <td>3.354633</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>[]</td>\n",
       "      <td>3.090159</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[]</td>\n",
       "      <td>3.059542</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>['penguin', 'antarctica']</td>\n",
       "      <td>2.865012</td>\n",
       "      <td>[penguin, antarctica]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.552083</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>['whales', 'beluga']</td>\n",
       "      <td>2.275912</td>\n",
       "      <td>[whales, beluga]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>['muhammedmuheisen']</td>\n",
       "      <td>2.239627</td>\n",
       "      <td>[muhammedmuheisen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.194461</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>['PathofthePanther', 'FloridaWild', 'Everglade...</td>\n",
       "      <td>2.169625</td>\n",
       "      <td>[pathofthepanther, floridawild, everglades, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.146791</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.092883</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.058058</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.002685</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>[]</td>\n",
       "      <td>2.000186</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>['pyramids', 'aerial', 'time', 'travel', 'Afri...</td>\n",
       "      <td>1.914131</td>\n",
       "      <td>[pyramids, aerial, time, travel, africa, egypt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>[]</td>\n",
       "      <td>1.875942</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>['climatechange', 'canada', 'sealpups']</td>\n",
       "      <td>1.815592</td>\n",
       "      <td>[climatechange, canada, sealpups]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>['onassignment', 'seals', 'harpseals', 'ice', ...</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>[onassignment, seals, harpseals, ice, canada, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>['wildlife', 'mountaingoats', 'commonground']</td>\n",
       "      <td>1.749957</td>\n",
       "      <td>[wildlife, mountaingoats, commonground]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>['muhammedmuheisen', 'Greece']</td>\n",
       "      <td>1.679880</td>\n",
       "      <td>[muhammedmuheisen, greece]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>['Kalahari']</td>\n",
       "      <td>1.650907</td>\n",
       "      <td>[kalahari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>['FollowMe', 'Norway', 'polarbear', 'family']</td>\n",
       "      <td>1.614290</td>\n",
       "      <td>[followme, norway, polarbear, family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>['followme', 'gratitude', 'narwhal', 'love', '...</td>\n",
       "      <td>1.608327</td>\n",
       "      <td>[followme, gratitude, narwhal, love, beauty, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>['rhinos', 'stoppoaching']</td>\n",
       "      <td>1.607644</td>\n",
       "      <td>[rhinos, stoppoaching]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>['FollowMe', 'BritishColumbia', 'jellyfish', '...</td>\n",
       "      <td>1.594303</td>\n",
       "      <td>[followme, britishcolumbia, jellyfish, salishsea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>['notrade']</td>\n",
       "      <td>1.541268</td>\n",
       "      <td>[notrade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>['fathers', 'costarica', 'fatherhood']</td>\n",
       "      <td>-0.605017</td>\n",
       "      <td>[fathers, costarica, fatherhood]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.605784</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>['simonroberts', 'merriealbion', 'britishlands...</td>\n",
       "      <td>-0.606267</td>\n",
       "      <td>[simonroberts, merriealbion, britishlandscape,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>['Philippines', 'FilipinoDiaspora']</td>\n",
       "      <td>-0.609415</td>\n",
       "      <td>[philippines, filipinodiaspora]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>['Libya']</td>\n",
       "      <td>-0.611697</td>\n",
       "      <td>[libya]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.612207</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>['sponsored']</td>\n",
       "      <td>-0.612681</td>\n",
       "      <td>[sponsored]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>['refugees', 'Venezuela', 'Brazil', 'Orinoco']</td>\n",
       "      <td>-0.615020</td>\n",
       "      <td>[refugees, venezuela, brazil, orinoco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>['alaga', 'FilipinoDiaspora']</td>\n",
       "      <td>-0.615324</td>\n",
       "      <td>[alaga, filipinodiaspora]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>['VisitWithRespect']</td>\n",
       "      <td>-0.616469</td>\n",
       "      <td>[visitwithrespect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>['MagnumPhotos']</td>\n",
       "      <td>-0.618118</td>\n",
       "      <td>[magnumphotos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>['louisarmstong', 'jazz', 'blackhistory']</td>\n",
       "      <td>-0.619200</td>\n",
       "      <td>[louisarmstong, jazz, blackhistory]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>['riyadh', 'saudiarabia', 'WWE']</td>\n",
       "      <td>-0.620118</td>\n",
       "      <td>[riyadh, saudiarabia, wwe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.623373</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.624957</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.627675</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>['MagnumPhotos']</td>\n",
       "      <td>-0.628327</td>\n",
       "      <td>[magnumphotos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>['stories', 'environment', 'agriculture', 'fut...</td>\n",
       "      <td>-0.628661</td>\n",
       "      <td>[stories, environment, agriculture, future, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>['refugees', 'Venezuela', 'Brazil', 'Orinoco',...</td>\n",
       "      <td>-0.629293</td>\n",
       "      <td>[refugees, venezuela, brazil, orinoco, warao]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>[]</td>\n",
       "      <td>-0.632137</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>['ancestralhealing']</td>\n",
       "      <td>-0.634680</td>\n",
       "      <td>[ancestralhealing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['happypills', 'pills', 'pharma', 'bigpharma']</td>\n",
       "      <td>-0.635760</td>\n",
       "      <td>[happypills, pills, pharma, bigpharma]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>['military', 'base', 'djibouti', 'china']</td>\n",
       "      <td>-0.636918</td>\n",
       "      <td>[military, base, djibouti, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>['nujeen', 'lookup', 'further']</td>\n",
       "      <td>-0.638357</td>\n",
       "      <td>[nujeen, lookup]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>['sponsored']</td>\n",
       "      <td>-0.649891</td>\n",
       "      <td>[sponsored]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>['sponsored', 'GivingTuesday', 'EdTech']</td>\n",
       "      <td>-0.671104</td>\n",
       "      <td>[sponsored, givingtuesday, edtech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>['sponsored', 'GivingTuesday', 'EdTech']</td>\n",
       "      <td>-0.672690</td>\n",
       "      <td>[sponsored, givingtuesday, edtech]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>['firstjob', 'job', 'work', 'china', 'jimei', ...</td>\n",
       "      <td>-0.679140</td>\n",
       "      <td>[firstjob, job, work, china, jimei, hotel, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>['onassignment']</td>\n",
       "      <td>-0.683392</td>\n",
       "      <td>[onassignment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>['environment', 'london', 'brexit', 'climatech...</td>\n",
       "      <td>-0.684928</td>\n",
       "      <td>[environment, london, brexit, climatechange, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              hashtags  Engagement_score  \\\n",
       "776                ['blackrhino', 'poachers', 'Ivory']         14.102355   \n",
       "836                                                 []         12.622301   \n",
       "330                                                 []          4.637615   \n",
       "542                                                 []          4.221715   \n",
       "161                                                 []          3.354633   \n",
       "338                                                 []          3.090159   \n",
       "170                                                 []          3.059542   \n",
       "46                           ['penguin', 'antarctica']          2.865012   \n",
       "850                                                 []          2.552083   \n",
       "603                               ['whales', 'beluga']          2.275912   \n",
       "281                               ['muhammedmuheisen']          2.239627   \n",
       "320                                                 []          2.194461   \n",
       "262  ['PathofthePanther', 'FloridaWild', 'Everglade...          2.169625   \n",
       "788                                                 []          2.146791   \n",
       "282                                                 []          2.092883   \n",
       "306                                                 []          2.058058   \n",
       "490                                                 []          2.002685   \n",
       "554                                                 []          2.000186   \n",
       "267  ['pyramids', 'aerial', 'time', 'travel', 'Afri...          1.914131   \n",
       "257                                                 []          1.875942   \n",
       "499            ['climatechange', 'canada', 'sealpups']          1.815592   \n",
       "572  ['onassignment', 'seals', 'harpseals', 'ice', ...          1.752563   \n",
       "787      ['wildlife', 'mountaingoats', 'commonground']          1.749957   \n",
       "374                     ['muhammedmuheisen', 'Greece']          1.679880   \n",
       "301                                       ['Kalahari']          1.650907   \n",
       "315      ['FollowMe', 'Norway', 'polarbear', 'family']          1.614290   \n",
       "514  ['followme', 'gratitude', 'narwhal', 'love', '...          1.608327   \n",
       "77                          ['rhinos', 'stoppoaching']          1.607644   \n",
       "292  ['FollowMe', 'BritishColumbia', 'jellyfish', '...          1.594303   \n",
       "521                                        ['notrade']          1.541268   \n",
       "..                                                 ...               ...   \n",
       "612             ['fathers', 'costarica', 'fatherhood']         -0.605017   \n",
       "969                                                 []         -0.605784   \n",
       "901  ['simonroberts', 'merriealbion', 'britishlands...         -0.606267   \n",
       "538                ['Philippines', 'FilipinoDiaspora']         -0.609415   \n",
       "407                                          ['Libya']         -0.611697   \n",
       "831                                                 []         -0.612207   \n",
       "745                                      ['sponsored']         -0.612681   \n",
       "768     ['refugees', 'Venezuela', 'Brazil', 'Orinoco']         -0.615020   \n",
       "426                      ['alaga', 'FilipinoDiaspora']         -0.615324   \n",
       "826                               ['VisitWithRespect']         -0.616469   \n",
       "769                                   ['MagnumPhotos']         -0.618118   \n",
       "25           ['louisarmstong', 'jazz', 'blackhistory']         -0.619200   \n",
       "712                   ['riyadh', 'saudiarabia', 'WWE']         -0.620118   \n",
       "774                                                 []         -0.623373   \n",
       "222                                                 []         -0.624957   \n",
       "920                                                 []         -0.627675   \n",
       "888                                   ['MagnumPhotos']         -0.628327   \n",
       "657  ['stories', 'environment', 'agriculture', 'fut...         -0.628661   \n",
       "832  ['refugees', 'Venezuela', 'Brazil', 'Orinoco',...         -0.629293   \n",
       "79                                                  []         -0.632137   \n",
       "485                               ['ancestralhealing']         -0.634680   \n",
       "0       ['happypills', 'pills', 'pharma', 'bigpharma']         -0.635760   \n",
       "743          ['military', 'base', 'djibouti', 'china']         -0.636918   \n",
       "495                    ['nujeen', 'lookup', 'further']         -0.638357   \n",
       "993                                      ['sponsored']         -0.649891   \n",
       "607           ['sponsored', 'GivingTuesday', 'EdTech']         -0.671104   \n",
       "605           ['sponsored', 'GivingTuesday', 'EdTech']         -0.672690   \n",
       "933  ['firstjob', 'job', 'work', 'china', 'jimei', ...         -0.679140   \n",
       "518                                   ['onassignment']         -0.683392   \n",
       "755  ['environment', 'london', 'brexit', 'climatech...         -0.684928   \n",
       "\n",
       "                                          clean_tokens  \n",
       "776                      [blackrhino, poachers, ivory]  \n",
       "836                                                 []  \n",
       "330                                                 []  \n",
       "542                                                 []  \n",
       "161                                                 []  \n",
       "338                                                 []  \n",
       "170                                                 []  \n",
       "46                               [penguin, antarctica]  \n",
       "850                                                 []  \n",
       "603                                   [whales, beluga]  \n",
       "281                                 [muhammedmuheisen]  \n",
       "320                                                 []  \n",
       "262  [pathofthepanther, floridawild, everglades, ra...  \n",
       "788                                                 []  \n",
       "282                                                 []  \n",
       "306                                                 []  \n",
       "490                                                 []  \n",
       "554                                                 []  \n",
       "267    [pyramids, aerial, time, travel, africa, egypt]  \n",
       "257                                                 []  \n",
       "499                  [climatechange, canada, sealpups]  \n",
       "572  [onassignment, seals, harpseals, ice, canada, ...  \n",
       "787            [wildlife, mountaingoats, commonground]  \n",
       "374                         [muhammedmuheisen, greece]  \n",
       "301                                         [kalahari]  \n",
       "315              [followme, norway, polarbear, family]  \n",
       "514  [followme, gratitude, narwhal, love, beauty, i...  \n",
       "77                              [rhinos, stoppoaching]  \n",
       "292  [followme, britishcolumbia, jellyfish, salishsea]  \n",
       "521                                          [notrade]  \n",
       "..                                                 ...  \n",
       "612                   [fathers, costarica, fatherhood]  \n",
       "969                                                 []  \n",
       "901  [simonroberts, merriealbion, britishlandscape,...  \n",
       "538                    [philippines, filipinodiaspora]  \n",
       "407                                            [libya]  \n",
       "831                                                 []  \n",
       "745                                        [sponsored]  \n",
       "768             [refugees, venezuela, brazil, orinoco]  \n",
       "426                          [alaga, filipinodiaspora]  \n",
       "826                                 [visitwithrespect]  \n",
       "769                                     [magnumphotos]  \n",
       "25                 [louisarmstong, jazz, blackhistory]  \n",
       "712                         [riyadh, saudiarabia, wwe]  \n",
       "774                                                 []  \n",
       "222                                                 []  \n",
       "920                                                 []  \n",
       "888                                     [magnumphotos]  \n",
       "657  [stories, environment, agriculture, future, cl...  \n",
       "832      [refugees, venezuela, brazil, orinoco, warao]  \n",
       "79                                                  []  \n",
       "485                                 [ancestralhealing]  \n",
       "0               [happypills, pills, pharma, bigpharma]  \n",
       "743                  [military, base, djibouti, china]  \n",
       "495                                   [nujeen, lookup]  \n",
       "993                                        [sponsored]  \n",
       "607                 [sponsored, givingtuesday, edtech]  \n",
       "605                 [sponsored, givingtuesday, edtech]  \n",
       "933  [firstjob, job, work, china, jimei, hotel, com...  \n",
       "518                                     [onassignment]  \n",
       "755  [environment, london, brexit, climatechange, m...  \n",
       "\n",
       "[997 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting the df\n",
    "insta_captions.sort_values(by='Engagement_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quantile, upper_quantile = insta_captions.Engagement_score.quantile([.25, .75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher = insta_captions[insta_captions['Engagement_score'] > upper_quantile]\n",
    "lower = insta_captions[insta_captions['Engagement_score'] > lower_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_reveal(high_df, low_df, num_topics = 5, num_words = 4):\n",
    "    dictionary = corpora.Dictionary(high_df['clean_tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in high_df['clean_tokens']]\n",
    "    pickle.dump(corpus, open('corpus_high.pkl', 'wb'))\n",
    "    NUM_TOPICS = num_topics\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    topics_high = ldamodel.print_topics(num_words=4)\n",
    "    print(\" For Upper Quartile, 5 topics are defined as follows: \")\n",
    "    for topic in topics_high:\n",
    "        print(topic)\n",
    "        \n",
    "    dictionary = corpora.Dictionary(low_df['clean_tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in low_df['clean_tokens']]\n",
    "    pickle.dump(corpus, open('corpus_low.pkl', 'wb'))\n",
    "    NUM_TOPICS = num_topics\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    topics_low = ldamodel.print_topics(num_words=num_words)\n",
    "    print(\" For Lower Quartile, 5 topics are defined as follows: \")\n",
    "    for topic in topics_low:\n",
    "        print(topic)\n",
    "    return topics_high, topics_low\n",
    "    print(type(topics_high))\n",
    "\n",
    "#     top_words_per_topic = []\n",
    "#     for t in range(lda_model.num_topics):\n",
    "#         top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 5)])\n",
    "\n",
    "#         pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For Upper Quartile, 5 topics are defined as follows: \n",
      "(0, '0.037*\"maasaimara\" + 0.025*\"daytonight\" + 0.025*\"stephenwilkes\" + 0.021*\"kenya\"')\n",
      "(1, '0.025*\"onassignment\" + 0.013*\"nature\" + 0.013*\"africa\" + 0.013*\"bigcatweekend\"')\n",
      "(2, '0.035*\"okavangolions\" + 0.023*\"kenya\" + 0.022*\"followme\" + 0.020*\"stoppoaching\"')\n",
      "(3, '0.099*\"followme\" + 0.057*\"antarctica\" + 0.035*\"penguin\" + 0.029*\"nature\"')\n",
      "(4, '0.032*\"wildlife\" + 0.032*\"commonground\" + 0.022*\"publiclands\" + 0.016*\"marinewildlife\"')\n",
      " For Lower Quartile, 5 topics are defined as follows: \n",
      "(0, '0.026*\"kenya\" + 0.026*\"stephenwilkes\" + 0.024*\"africa\" + 0.021*\"nature\"')\n",
      "(1, '0.043*\"followme\" + 0.027*\"nature\" + 0.026*\"wildlife\" + 0.017*\"muhammedmuheisen\"')\n",
      "(2, '0.029*\"followme\" + 0.018*\"everydayrefugees\" + 0.010*\"wildlifephotography\" + 0.007*\"onassignment\"')\n",
      "(3, '0.029*\"antarctica\" + 0.022*\"followme\" + 0.022*\"commonground\" + 0.013*\"publiclands\"')\n",
      "(4, '0.033*\"floridawild\" + 0.020*\"pathofthepanther\" + 0.016*\"japan\" + 0.016*\"keepflwild\"')\n"
     ]
    }
   ],
   "source": [
    "t = topics_reveal(higher, lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring topics into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "num_words = 4\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(higher['clean_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in higher['clean_tokens']]\n",
    "pickle.dump(corpus, open('corpus_high.pkl', 'wb'))\n",
    "UM_TOPICS = num_topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics_high = ldamodel.print_topics(num_words=4)    \n",
    "\n",
    "top_words_per_topic = []\n",
    "for t in range(ldamodel.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = 5)])\n",
    "\n",
    "    high_q = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P'])    \n",
    "\n",
    "        \n",
    "dictionary = corpora.Dictionary(lower['clean_tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in lower['clean_tokens']]\n",
    "pickle.dump(corpus, open('corpus_low.pkl', 'wb'))\n",
    "NUM_TOPICS = num_topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics_low = ldamodel.print_topics(num_words=num_words) \n",
    "\n",
    "top_words_per_topic = []\n",
    "for t in range(ldamodel.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in ldamodel.show_topic(t, topn = 5)])\n",
    "\n",
    "    low_q = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']) \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the overall mean for a topic\n",
    "\n",
    "low_group = low_q.groupby(['Word']).mean()\n",
    "high_group = high_q.groupby(['Word']).mean()\n",
    "low_group.drop('Topic', axis=1, inplace=True)\n",
    "high_group.drop('Topic', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the difference between the upper and lower quartile\n",
    "\n",
    "table = high_group.merge(low_group, on='Word', suffixes=('_high', '_low'))\n",
    "table['difference'] = (table['P_high']-table['P_low'])\n",
    "table['abs_dif'] = abs(table['difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_high</th>\n",
       "      <th>P_low</th>\n",
       "      <th>difference</th>\n",
       "      <th>abs_dif</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antarctica</th>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.019529</td>\n",
       "      <td>0.039289</td>\n",
       "      <td>0.039289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>followme</th>\n",
       "      <td>0.047489</td>\n",
       "      <td>0.025868</td>\n",
       "      <td>0.021620</td>\n",
       "      <td>0.021620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penguin</th>\n",
       "      <td>0.040248</td>\n",
       "      <td>0.019692</td>\n",
       "      <td>0.020556</td>\n",
       "      <td>0.020556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>0.026782</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.010756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okavangolions</th>\n",
       "      <td>0.032989</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.010727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stephenwilkes</th>\n",
       "      <td>0.016113</td>\n",
       "      <td>0.025773</td>\n",
       "      <td>-0.009660</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commonground</th>\n",
       "      <td>0.024149</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.008638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daytonight</th>\n",
       "      <td>0.016104</td>\n",
       "      <td>0.024373</td>\n",
       "      <td>-0.008269</td>\n",
       "      <td>0.008269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conservation</th>\n",
       "      <td>0.021594</td>\n",
       "      <td>0.013861</td>\n",
       "      <td>0.007734</td>\n",
       "      <td>0.007734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildlifephotography</th>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.006973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simonnorfolk</th>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>-0.005217</td>\n",
       "      <td>0.005217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigcats</th>\n",
       "      <td>0.016424</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.004968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>documentaryphotography</th>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.022494</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildlife</th>\n",
       "      <td>0.026789</td>\n",
       "      <td>0.027213</td>\n",
       "      <td>-0.000424</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          P_high     P_low  difference   abs_dif\n",
       "Word                                                            \n",
       "antarctica              0.058818  0.019529    0.039289  0.039289\n",
       "followme                0.047489  0.025868    0.021620  0.021620\n",
       "penguin                 0.040248  0.019692    0.020556  0.020556\n",
       "nature                  0.026782  0.016026    0.010756  0.010756\n",
       "okavangolions           0.032989  0.022262    0.010727  0.010727\n",
       "stephenwilkes           0.016113  0.025773   -0.009660  0.009660\n",
       "commonground            0.024149  0.015511    0.008638  0.008638\n",
       "daytonight              0.016104  0.024373   -0.008269  0.008269\n",
       "conservation            0.021594  0.013861    0.007734  0.007734\n",
       "wildlifephotography     0.018438  0.011465    0.006973  0.006973\n",
       "simonnorfolk            0.021155  0.026372   -0.005217  0.005217\n",
       "bigcats                 0.016424  0.011456    0.004968  0.004968\n",
       "documentaryphotography  0.021121  0.022494   -0.001373  0.001373\n",
       "wildlife                0.026789  0.027213   -0.000424  0.000424"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.sort_values('abs_dif', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation: To increase engagement, NatGeo should make posts with its top photographers showing natural landscapes, specifically the Artic and African landscapes with its wildlife."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
